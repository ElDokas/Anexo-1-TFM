{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANEXO 1. TFM GONZALO FONTÁN DE LA CRUZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de los diferentes procesos de tokenización que existen, y que posteriormente pasaremos a desgranar, el que mejor ayuda a comprender en qué consiste este mecanismo es el que divide los fragmentos de texto en caracteres únicos.\n",
    "Para poder realizarlo nosotros mismos de tal manera que un fragmento de texto inmenso podamos dividirlo en caracteres a través de Python nos vamos a servir de uno de los módulos más útiles: expresiones regulares o regular expression en inglés (re/regex). A través de expresiones regulares que son una serie de comandos que incluimos en la función podemos pedirle al ordenador que segmente las oraciones de tal manera que se divida en pequeños fragmentos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola,', ' ', 'esto', ' ', 'es', ' ', 'un', ' ', 'texto!']\n"
     ]
    }
   ],
   "source": [
    "import re #Importamos la librería\n",
    "txt = \"Hola, esto es un texto!\" #Creamos una variable con el texto que queremos trocear\n",
    "resultado = re.split(r'(\\s)', txt) #Esta línea la comentamos en el siguiente recuadro\n",
    "print (resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una variable en la que guardamos el resultado de dividir a través de la función split con la expresión regular <r'(\\s)'> que hace al sistema buscar cualquier espacio para así dividirlo a través de split usando los espacios como criterio.\n",
    "\n",
    "Las expresiones regulares son muy flexibles y se pueden aprender a generar sin necesidad de una máquina, pero ya que el presente proyecto se basa en explicar los largos modelos de lenguaje cabe mencionar que estos son expertos en producir regex.\n",
    "\n",
    "El problema con este primer caso es que como se puede observar se generan elementos dentro de la variable que incluyen comas como puede ser el inicial 'Hola,', para ello tenemos que completar la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', '', ' ', 'mundo', ',', '', ' ', 'este', ' ', 'es', ' ', 'el', ' ', 'segundo', ' ', 'texto', ' ', 'del', ' ', 'trabajo']\n"
     ]
    }
   ],
   "source": [
    "txt2 = \"Hola, mundo, este es el segundo texto del trabajo\"\n",
    "resultado2 = re.split(r'([,.]|\\s)', txt2)\n",
    "print (resultado2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, en este caso la expresión regular cuenta con más elementos r'([,.]|\\s)' y en este caso se incluyen también elementos como comas y puntos para que no se incluyan dentro de los elementos que hemos dividido.\n",
    "\n",
    "Ahora nos surge un nuevo reto ya que como podemos observar los espacios se incluyen como elementos de la lista, esto se puede solucionar muy facilmente con dos opciones:\n",
    "\n",
    "1. Iterar sobre la secuencia que hemos generado de tal manera que no incluyamos aquellos elementos que sean espacios o elementos vacíos.\n",
    "2. Emplear la función .strip, un método .strip() se usa para eliminar los espacios en blanco (espacios, tabulaciones, saltos de línea) al inicio y al final de una cadena.\n",
    "\n",
    "Ambas opciones son muy instructivas y las podemos abordar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', 'mundo', ',', 'este', 'es', 'el', 'segundo', 'texto', 'del', 'trabajo']\n"
     ]
    }
   ],
   "source": [
    "#quitar los espacios iterando\n",
    "opcion1 =[]\n",
    "for i in resultado2:\n",
    "    if i != \" \" and i != \"\":\n",
    "        opcion1.append(i)\n",
    "print(opcion1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', 'mundo', ',', 'este', 'es', 'el', 'segundo', 'texto', 'del', 'trabajo']\n"
     ]
    }
   ],
   "source": [
    "#quitar los espacios a través de strip\n",
    "opcion2 = [i for i in resultado2 if i.strip()]\n",
    "print(opcion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante contemplar todas las posibilidades en forma de signos de puntuación dentro de nuestra expresión regular de tal manera que se consiga una limpieza absoluta del texto.\n",
    "\n",
    "Para ello vamos a emplear los mismos elementos que hemos usado anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', 'gente', '--', '.', 'Esto', '!', ',', 'es', 'un', 'notebook', 'explicativo', '.']\n"
     ]
    }
   ],
   "source": [
    "txt3 = \"Hola, gente--. Esto!, es un notebook explicativo.\"\n",
    "resultado3 = re.split(r'([,.:;?_!\"()\\']|--|\\s)', txt3) #Simplemente añadimos en la expresión regular todos esos elementos que queremos que incluya por separado\n",
    "resultado3 = [i for i in resultado3 if i.strip()]\n",
    "print (resultado3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que contamos con herramientas suficientes para dividir un texto en tokens podemos probar a coger un fragmento de El Quijote para dividirlo de tal manera que consigamos tokens a raíz de él.\n",
    "\n",
    "Para ello podemos usar varías rutas:\n",
    "1. Cargar el libro en este archivo (lo haremos posteriormente)\n",
    "2. Crear una variable con parte del texto como ya hemos hecho antes y trabajar sobre ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote = \"\"\"Desocupado lector, sin juramento me podrás creer que quisiera que este libro, como hijo del entendimiento, fuera el más hermoso, el más gallardo y más discreto que pudiera imaginarse. Pero no he podido yo contravenir al orden de naturaleza; que en ella cada cosa engendra su semejante. Y así, ¿qué podrá engendrar el estéril y mal cultivado ingenio mío, sino la historia de un hijo seco, avellanado, antojadizo y lleno de pensamientos varios y nunca imaginados de otro alguno, bien como quien se engendró en una cárcel, donde toda incomodidad tiene su asiento y donde todo triste ruido hace su habitación? El sosiego, el lugar apacible, la amenidad de los campos, la serenidad de los cielos, el murmurar de las fuentes, la quietud del espíritu son grande parte para que las musas más estériles se muestren fecundas y ofrezcan partos al mundo que le colmen de maravilla y de contento. Acontece tener un padre un hijo feo y sin gracia alguna, y el amor que le tiene le pone una venda en los ojos para que no vea sus faltas, antes las juzga por discreciones y lindezas y las cuenta a sus amigos por agudezas y donaires.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Desocupado', ' ', 'lector', ',', '', ' ', 'sin', ' ', 'juramento', ' ', 'me', ' ', 'podrás', ' ', 'creer', ' ', 'que', ' ', 'quisiera', ' ', 'que', ' ', 'este', ' ', 'libro', ',', '', ' ', 'como', ' ', 'hijo', ' ', 'del', ' ', 'entendimiento', ',', '', ' ', 'fuera', ' ', 'el', ' ', 'más', ' ', 'hermoso', ',', '', ' ', 'el', ' ', 'más', ' ', 'gallardo', ' ', 'y', ' ', 'más', ' ', 'discreto', ' ', 'que', ' ', 'pudiera', ' ', 'imaginarse', '.', '', ' ', 'Pero', ' ', 'no', ' ', 'he', ' ', 'podido', ' ', 'yo', ' ', 'contravenir', ' ', 'al', ' ', 'orden', ' ', 'de', ' ', 'naturaleza', ';', '', ' ', 'que', ' ', 'en', ' ', 'ella', ' ', 'cada', ' ', 'cosa', ' ', 'engendra', ' ', 'su', ' ', 'semejante', '.', '', ' ', 'Y', ' ', 'así', ',', '', ' ', '¿qué', ' ', 'podrá', ' ', 'engendrar', ' ', 'el', ' ', 'estéril', ' ', 'y', ' ', 'mal', ' ', 'cultivado', ' ', 'ingenio', ' ', 'mío', ',', '', ' ', 'sino', ' ', 'la', ' ', 'historia', ' ', 'de', ' ', 'un', ' ', 'hijo', ' ', 'seco', ',', '', ' ', 'avellanado', ',', '', ' ', 'antojadizo', ' ', 'y', ' ', 'lleno', ' ', 'de', ' ', 'pensamientos', ' ', 'varios', ' ', 'y', ' ', 'nunca', ' ', 'imaginados', ' ', 'de', ' ', 'otro', ' ', 'alguno', ',', '', ' ', 'bien', ' ', 'como', ' ', 'quien', ' ', 'se', ' ', 'engendró', ' ', 'en', ' ', 'una', ' ', 'cárcel', ',', '', ' ', 'donde', ' ', 'toda', ' ', 'incomodidad', ' ', 'tiene', ' ', 'su', ' ', 'asiento', ' ', 'y', ' ', 'donde', ' ', 'todo', ' ', 'triste', ' ', 'ruido', ' ', 'hace', ' ', 'su', ' ', 'habitación', '?', '', ' ', 'El', ' ', 'sosiego', ',', '', ' ', 'el', ' ', 'lugar', ' ', 'apacible', ',', '', ' ', 'la', ' ', 'amenidad', ' ', 'de', ' ', 'los', ' ', 'campos', ',', '', ' ', 'la', ' ', 'serenidad', ' ', 'de', ' ', 'los', ' ', 'cielos', ',', '', ' ', 'el', ' ', 'murmurar', ' ', 'de', ' ', 'las', ' ', 'fuentes', ',', '', ' ', 'la', ' ', 'quietud', ' ', 'del', ' ', 'espíritu', ' ', 'son', ' ', 'grande', ' ', 'parte', ' ', 'para', ' ', 'que', ' ', 'las', ' ', 'musas', ' ', 'más', ' ', 'estériles', ' ', 'se', ' ', 'muestren', ' ', 'fecundas', ' ', 'y', ' ', 'ofrezcan', ' ', 'partos', ' ', 'al', ' ', 'mundo', ' ', 'que', ' ', 'le', ' ', 'colmen', ' ', 'de', ' ', 'maravilla', ' ', 'y', ' ', 'de', ' ', 'contento', '.', '', ' ', 'Acontece', ' ', 'tener', ' ', 'un', ' ', 'padre', ' ', 'un', ' ', 'hijo', ' ', 'feo', ' ', 'y', ' ', 'sin', ' ', 'gracia', ' ', 'alguna', ',', '', ' ', 'y', ' ', 'el', ' ', 'amor', ' ', 'que', ' ', 'le', ' ', 'tiene', ' ', 'le', ' ', 'pone', ' ', 'una', ' ', 'venda', ' ', 'en', ' ', 'los', ' ', 'ojos', ' ', 'para', ' ', 'que', ' ', 'no', ' ', 'vea', ' ', 'sus', ' ', 'faltas', ',', '', ' ', 'antes', ' ', 'las', ' ', 'juzga', ' ', 'por', ' ', 'discreciones', ' ', 'y', ' ', 'lindezas', ' ', 'y', ' ', 'las', ' ', 'cuenta', ' ', 'a', ' ', 'sus', ' ', 'amigos', ' ', 'por', ' ', 'agudezas', ' ', 'y', ' ', 'donaires', '.', '']\n"
     ]
    }
   ],
   "source": [
    "quijotedividido = re.split(r'([,.:;?_!\"()\\']|--|\\s)', quijote)\n",
    "result = [i for i in quijotedividido if i.strip()]\n",
    "print(quijotedividido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar a través de la función len() que se encarga de contabilizar cuán larga es una lista la cantidad de elementos que tenemos en nuestro fragmento dividido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439\n"
     ]
    }
   ],
   "source": [
    "print(len(quijotedividido))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar estamos generando un vocabulario con un total de 439 tokens (en este caso palabras para simplificar un poco la forma en la que se comprende el formato) y esto nos puede servir para hacernos a una idea del funcionamiento del vocabulario. Ahora, una vez tenemos el vocabulario a través de crer con una lista de comprensión el vocabulario reunido en una variable que sea reconocible para nosotros (vocab) para después iterar y poder enumerarlo de modo que asociemos cada token a ese número único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Desocupado\n",
      "1  \n",
      "2 lector\n",
      "3 ,\n",
      "4 \n",
      "5 sin\n",
      "6 juramento\n",
      "7 me\n",
      "8 podrás\n",
      "9 creer\n",
      "10 que\n",
      "11 quisiera\n",
      "12 este\n",
      "13 libro\n",
      "14 como\n",
      "15 hijo\n",
      "16 del\n",
      "17 entendimiento\n",
      "18 fuera\n",
      "19 el\n",
      "20 más\n",
      "21 hermoso\n",
      "22 gallardo\n",
      "23 y\n",
      "24 discreto\n",
      "25 pudiera\n",
      "26 imaginarse\n",
      "27 .\n",
      "28 Pero\n",
      "29 no\n",
      "30 he\n",
      "31 podido\n",
      "32 yo\n",
      "33 contravenir\n",
      "34 al\n",
      "35 orden\n",
      "36 de\n",
      "37 naturaleza\n",
      "38 ;\n",
      "39 en\n",
      "40 ella\n",
      "41 cada\n",
      "42 cosa\n",
      "43 engendra\n",
      "44 su\n",
      "45 semejante\n",
      "46 Y\n",
      "47 así\n",
      "48 ¿qué\n",
      "49 podrá\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(quijotedividido)}\n",
    "for i, x in enumerate(vocab):\n",
    "    if i < 50:\n",
    "            print(i, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a instalar gensim que en resumidad cuentas es una biblioteca especializada en modelado de temas y representaciones vectoriales del lenguaje. Permite cargar modelos preentrenados como Word2Vec, FastText o GloVe, lo que facilita el análisis semántico y de similitud entre palabras, aspectos clave en lingüística computacional. A nosotros nos interesa en este caso Word2Vec para poder ver cómo se aplica la vectorización y cómo se captura la información de las palabras y sus contextos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\gonzalo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\gonzalo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/24.0 MB 7.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 8.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/24.0 MB 8.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.3/24.0 MB 8.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.7/24.0 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.8/24.0 MB 9.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.2/24.0 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.5/24.0 MB 10.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.8/24.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.7/24.0 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.4/15.8 MB 12.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 7.3/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/46.2 MB 11.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.7/46.2 MB 11.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 7.1/46.2 MB 11.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 9.4/46.2 MB 11.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 11.8/46.2 MB 11.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 14.4/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 16.8/46.2 MB 11.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 19.4/46.2 MB 11.8 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 22.0/46.2 MB 11.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 24.4/46.2 MB 11.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 27.0/46.2 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.4/46.2 MB 11.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 31.7/46.2 MB 11.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 34.3/46.2 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.7/46.2 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 39.1/46.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.7/46.2 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.0/46.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 11.5 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Gonzalo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vamos a coger, una vez hemos asociado el modelo a una variable, un par de palabras de uso más común y popular (Messi en nuestro caso, ya que se puede intuir que el lector cuenta con los conocimientos como para poder comprenderlos). Los asociamos al modelo para poder ver sus vectores, pero en este caso el lector puede cambiar sus palabras a su antoja e iterar para poder ver los conceptos que desee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vec = model\n",
    "Messi = w_vec['messi']\n",
    "Argentina = w_vec['argentina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a indagar el vector asociado a Messi, como se va a poder observar es un vector muy amplio (en este caso su dimensión es de 300 y se puede comprobar abriendo una celda y poniendo len(messi) dentro de ella). Esta es la longitud de dimensión de la que se habla en el documento y es la que permite que el modelo capte la semántica y las relaciones de las palabras, a mayor dimensión, mejor captación de los conceptos. Esta comprobación se puede hacer con cualquier palabra que se desee (siempre y cuando estén dentro del modelo que descarguemos, ya que están preentrenadas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09960938, -0.04956055,  0.33007812,  0.36914062,  0.08203125,\n",
       "        0.06396484,  0.0213623 , -0.22851562,  0.01000977, -0.02258301,\n",
       "       -0.17773438, -0.22363281, -0.23046875, -0.03369141, -0.08984375,\n",
       "        0.18847656,  0.01525879,  0.3046875 , -0.08642578, -0.1640625 ,\n",
       "       -0.14941406, -0.25585938,  0.38085938,  0.09521484,  0.11962891,\n",
       "       -0.09130859, -0.07421875, -0.24804688,  0.09033203,  0.13085938,\n",
       "       -0.07470703,  0.13867188,  0.02868652, -0.09716797, -0.24609375,\n",
       "        0.171875  , -0.1796875 ,  0.09423828,  0.17089844,  0.17089844,\n",
       "       -0.01464844, -0.22753906,  0.4453125 ,  0.35742188,  0.35742188,\n",
       "       -0.15722656,  0.23925781, -0.28515625, -0.3046875 ,  0.3046875 ,\n",
       "       -0.10449219,  0.21679688, -0.08984375, -0.12988281, -0.02026367,\n",
       "        0.17578125, -0.0201416 ,  0.08691406, -0.13867188,  0.08154297,\n",
       "       -0.09375   ,  0.22558594, -0.22949219, -0.2421875 ,  0.23046875,\n",
       "       -0.10742188,  0.11279297, -0.09130859,  0.24414062,  0.1484375 ,\n",
       "        0.12255859,  0.34960938, -0.00765991,  0.04589844, -0.40625   ,\n",
       "       -0.03393555, -0.08984375, -0.30664062, -0.17382812, -0.3125    ,\n",
       "       -0.1484375 , -0.02478027, -0.16601562, -0.14160156,  0.12011719,\n",
       "        0.05834961,  0.18066406,  0.18066406, -0.10449219, -0.05957031,\n",
       "        0.17089844,  0.00366211, -0.35546875,  0.13574219,  0.01647949,\n",
       "       -0.15429688, -0.3359375 , -0.16796875,  0.23242188, -0.11767578,\n",
       "        0.03710938,  0.24707031,  0.26171875,  0.328125  , -0.03222656,\n",
       "        0.14257812, -0.07421875, -0.19238281,  0.17578125, -0.08105469,\n",
       "        0.01855469, -0.12890625,  0.23730469, -0.03393555,  0.04345703,\n",
       "       -0.06542969, -0.3984375 , -0.33203125, -0.11132812, -0.21679688,\n",
       "        0.2265625 , -0.27734375,  0.1328125 ,  0.16699219,  0.04321289,\n",
       "       -0.28710938, -0.39453125,  0.04663086, -0.12353516, -0.09619141,\n",
       "       -0.17773438, -0.04467773,  0.06787109, -0.02172852,  0.15136719,\n",
       "       -0.23339844, -0.23242188, -0.05615234,  0.00218201,  0.10595703,\n",
       "        0.16308594, -0.32226562,  0.14257812, -0.06835938, -0.04418945,\n",
       "        0.04858398, -0.23046875, -0.01574707,  0.04125977, -0.32226562,\n",
       "        0.25      , -0.00144196, -0.29296875, -0.16699219, -0.14257812,\n",
       "       -0.19140625,  0.00994873,  0.19238281,  0.07958984, -0.21875   ,\n",
       "        0.20800781,  0.26757812,  0.01904297, -0.08740234, -0.04223633,\n",
       "       -0.24902344,  0.27734375, -0.09472656, -0.390625  ,  0.26757812,\n",
       "       -0.5234375 ,  0.1015625 ,  0.07763672,  0.03222656, -0.13769531,\n",
       "        0.10595703,  0.44140625, -0.5390625 ,  0.16503906, -0.27929688,\n",
       "        0.15625   ,  0.14257812, -0.04858398, -0.01062012, -0.09765625,\n",
       "        0.15332031, -0.16503906, -0.14746094,  0.00616455,  0.01916504,\n",
       "       -0.03735352,  0.10107422, -0.1328125 , -0.12158203,  0.5390625 ,\n",
       "       -0.05883789, -0.15917969,  0.15234375, -0.16503906, -0.41601562,\n",
       "       -0.44921875, -0.13867188, -0.01647949, -0.18359375, -0.01196289,\n",
       "       -0.46484375,  0.03857422, -0.36523438, -0.07080078, -0.04125977,\n",
       "        0.05786133,  0.23925781, -0.453125  , -0.04980469, -0.27539062,\n",
       "        0.21972656,  0.14746094, -0.1875    , -0.19921875,  0.19238281,\n",
       "       -0.08105469, -0.03808594,  0.00393677,  0.25390625,  0.11328125,\n",
       "        0.15625   ,  0.30664062,  0.00096512,  0.04003906, -0.25      ,\n",
       "       -0.1328125 ,  0.06591797, -0.01574707,  0.07568359,  0.13867188,\n",
       "       -0.10693359, -0.16210938, -0.04077148,  0.12695312,  0.23632812,\n",
       "        0.30078125,  0.08642578,  0.01501465,  0.01556396, -0.38867188,\n",
       "        0.06494141, -0.27929688,  0.07080078, -0.24316406, -0.13476562,\n",
       "        0.27734375,  0.00994873,  0.21972656,  0.44921875,  0.17480469,\n",
       "       -0.24511719, -0.05444336,  0.02990723, -0.08251953,  0.10058594,\n",
       "       -0.02734375,  0.07617188,  0.20214844, -0.09179688,  0.03735352,\n",
       "       -0.01062012, -0.22363281, -0.04101562, -0.01220703,  0.12011719,\n",
       "       -0.30859375, -0.05175781, -0.23339844,  0.09423828,  0.25195312,\n",
       "        0.06689453,  0.13574219, -0.00485229, -0.13183594,  0.29101562,\n",
       "       -0.13085938,  0.02990723,  0.09765625,  0.10058594, -0.22949219,\n",
       "       -0.05810547,  0.23730469,  0.09619141,  0.12207031, -0.05786133,\n",
       "        0.2890625 ,  0.32617188, -0.12158203,  0.03173828, -0.15527344,\n",
       "        0.30664062, -0.15625   , -0.02233887, -0.08105469,  0.18554688],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Messi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto es donde vamos a poder comprender a la perfección cuál es el funcionamiento de la vectorización de forma más clara. Si se coge el vector Messi, que es el que hemos investigado más en profundidad, se le suma el vector Portugal y se le resta el vector Argentina podemos ver que la palabra con mayor similaridad que surge de dicha operación no es otra que la que podríamos esperar: Ronaldo. Este pequeño juego interactivo se puede probar con cualquier palabra, pero se recomienda tener en cuenta que este modelo previamente entrenado no cuenta con todo el vocabulario que se puede esperar, por lo que deberíamos limitar nuestras expectativas de cara a probar con ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ronaldo', 0.705854594707489), ('Xavi', 0.7020642161369324), ('Iniesta', 0.6866832375526428), ('Fabregas', 0.6855268478393555), ('Drogba', 0.6765247583389282), ('Cristiano_Ronaldo', 0.6650602221488953), (\"Eto'o\", 0.6632802486419678), ('Ibrahimovic', 0.6594337224960327), ('Cesc_Fabregas', 0.6520246267318726), ('Puyol', 0.650626003742218)]\n"
     ]
    }
   ],
   "source": [
    "print(w_vec.most_similar(positive=['Messi', 'Portugal'], negative=['Argentina'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"git\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!git remote add origin https://github.com/ElDokas/Anexo-1-TFM.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
